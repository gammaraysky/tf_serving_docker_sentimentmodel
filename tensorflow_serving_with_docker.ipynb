{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why use docker/tensorflow serving?\n",
    "- flask etc, upon single user's inference, locks up the server cpu/gpu and other users have to wait\n",
    "- flask cannot load large models, eg computer vision model with 1gb weights\n",
    "- extremely cumbersome to rewrite code for each flask deployment/model updates and onboard other team members.\n",
    "- with tf serving, you can export new models to the deployment folder and tf serving will automatically update to the latest detected saved model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's begin with a simple sentiment analysis model on an amazon reviews dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train.py\n"
     ]
    }
   ],
   "source": [
    "# %%writefile -a train.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "float_formatter = \"{:.2f}\".format\n",
    "np.set_printoptions(formatter={'float_kind':float_formatter})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###? IN LINUX YOU CAN READ THE FIRST FEW LINES OF THE CSV FROM CLI, UNFORTUNATELY NO EQUIVALENT IN WINDOWS.\n",
    "# !head -n 2 ./data/amazonfinefoodreviews/Reviews.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to train.py\n"
     ]
    }
   ],
   "source": [
    "# %%writefile -a train.py \n",
    "\n",
    "def load_dataset(filepath, num_samples):\n",
    "    df = pd.read_csv(filepath, usecols=[6,9], nrows=num_samples)\n",
    "    df.columns = ['rating','text']\n",
    "    df['label'] = df['rating'].apply(lambda x: 1 if x>=4 else 0 if x==3 else -1)\n",
    "\n",
    "    text = df['text'].tolist()\n",
    "    # text = [str(t).encode('ascii', 'replace') for t in text]\n",
    "    text = np.array(text, dtype=object)\n",
    "\n",
    "    labels = df['label'].tolist()\n",
    "    labels = np.array(pd.get_dummies(labels), dtype=int)[:]\n",
    "\n",
    "    return labels, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1] I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.\n",
      "[1 0] Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".\n",
      "[0 1] This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.\n",
      "[1 0] If you are looking for the secret ingredient in Robitussin I believe I have found it.  I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda.  The flavor is very medicinal.\n",
      "[0 1] Great taffy at a great price.  There was a wide assortment of yummy taffy.  Delivery was very quick.  If your a taffy lover, this is a deal.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###? LOAD DATASET\n",
    "labels, text = load_dataset(filepath='./data/amazonfinefoodreviews/Reviews.csv', num_samples=5)\n",
    "\n",
    "for lbl,txt in list(zip(labels, text)):\n",
    "    print(lbl,txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We'll use a pre-trained model from tfhub.dev \n",
    "- these pretrained NNs return 50 dims and 128 dims respectively. go to tfhub.dev to browse other available models.\n",
    "- https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\n",
    "- https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to train.py\n"
     ]
    }
   ],
   "source": [
    "# %%writefile -a train.py \n",
    "\n",
    "def get_model():\n",
    "    hub_layer = hub.KerasLayer('https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1', output_shape=[128], input_shape=[], dtype=tf.string, name='input', trainable=False)\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(hub_layer)\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(3, activation='softmax', name='output'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The pre-trained model already does text preprocessing for us. Let's get a sense of the embeddings it produces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.225497</td>\n",
       "      <td>-0.097742</td>\n",
       "      <td>0.230630</td>\n",
       "      <td>0.139735</td>\n",
       "      <td>0.086153</td>\n",
       "      <td>-0.045874</td>\n",
       "      <td>0.071118</td>\n",
       "      <td>-0.160892</td>\n",
       "      <td>0.112284</td>\n",
       "      <td>0.011471</td>\n",
       "      <td>-0.007685</td>\n",
       "      <td>-0.075731</td>\n",
       "      <td>-0.092246</td>\n",
       "      <td>-0.120212</td>\n",
       "      <td>-0.040878</td>\n",
       "      <td>0.188590</td>\n",
       "      <td>0.045904</td>\n",
       "      <td>0.006215</td>\n",
       "      <td>-0.124716</td>\n",
       "      <td>0.082918</td>\n",
       "      <td>0.118537</td>\n",
       "      <td>0.050865</td>\n",
       "      <td>-0.061020</td>\n",
       "      <td>-0.034868</td>\n",
       "      <td>0.024469</td>\n",
       "      <td>0.085191</td>\n",
       "      <td>0.109251</td>\n",
       "      <td>-0.024774</td>\n",
       "      <td>0.011921</td>\n",
       "      <td>0.146256</td>\n",
       "      <td>-0.037281</td>\n",
       "      <td>0.080321</td>\n",
       "      <td>-0.095374</td>\n",
       "      <td>-0.043692</td>\n",
       "      <td>0.136369</td>\n",
       "      <td>0.042527</td>\n",
       "      <td>0.053423</td>\n",
       "      <td>-0.100668</td>\n",
       "      <td>-0.034157</td>\n",
       "      <td>0.115415</td>\n",
       "      <td>-0.135856</td>\n",
       "      <td>-0.059653</td>\n",
       "      <td>-0.008103</td>\n",
       "      <td>0.105087</td>\n",
       "      <td>0.045498</td>\n",
       "      <td>-0.033328</td>\n",
       "      <td>-0.006868</td>\n",
       "      <td>0.004209</td>\n",
       "      <td>0.091305</td>\n",
       "      <td>-0.003479</td>\n",
       "      <td>-0.033370</td>\n",
       "      <td>0.105743</td>\n",
       "      <td>-0.112079</td>\n",
       "      <td>0.068110</td>\n",
       "      <td>-0.041359</td>\n",
       "      <td>-0.018282</td>\n",
       "      <td>-0.132670</td>\n",
       "      <td>0.016037</td>\n",
       "      <td>-0.096326</td>\n",
       "      <td>-0.003783</td>\n",
       "      <td>-0.059288</td>\n",
       "      <td>-0.059012</td>\n",
       "      <td>0.078189</td>\n",
       "      <td>-0.089968</td>\n",
       "      <td>0.046925</td>\n",
       "      <td>0.117595</td>\n",
       "      <td>-0.164911</td>\n",
       "      <td>-0.042599</td>\n",
       "      <td>0.097080</td>\n",
       "      <td>0.028123</td>\n",
       "      <td>-0.200099</td>\n",
       "      <td>0.088148</td>\n",
       "      <td>0.027126</td>\n",
       "      <td>-0.029836</td>\n",
       "      <td>0.045751</td>\n",
       "      <td>0.100097</td>\n",
       "      <td>0.051409</td>\n",
       "      <td>-0.131577</td>\n",
       "      <td>0.039602</td>\n",
       "      <td>-0.101337</td>\n",
       "      <td>0.018500</td>\n",
       "      <td>-0.029483</td>\n",
       "      <td>0.025566</td>\n",
       "      <td>-0.152488</td>\n",
       "      <td>0.136041</td>\n",
       "      <td>-0.041233</td>\n",
       "      <td>-0.017325</td>\n",
       "      <td>0.068525</td>\n",
       "      <td>0.108027</td>\n",
       "      <td>0.191753</td>\n",
       "      <td>0.034966</td>\n",
       "      <td>0.055814</td>\n",
       "      <td>-0.058237</td>\n",
       "      <td>-0.012290</td>\n",
       "      <td>0.030932</td>\n",
       "      <td>0.137788</td>\n",
       "      <td>0.047096</td>\n",
       "      <td>-0.037614</td>\n",
       "      <td>-0.042310</td>\n",
       "      <td>0.137921</td>\n",
       "      <td>0.164177</td>\n",
       "      <td>0.104449</td>\n",
       "      <td>0.124679</td>\n",
       "      <td>-0.109762</td>\n",
       "      <td>-0.015248</td>\n",
       "      <td>-0.188383</td>\n",
       "      <td>-0.210898</td>\n",
       "      <td>0.028174</td>\n",
       "      <td>-0.034462</td>\n",
       "      <td>-0.093645</td>\n",
       "      <td>-0.068922</td>\n",
       "      <td>-0.084514</td>\n",
       "      <td>-0.024698</td>\n",
       "      <td>0.107229</td>\n",
       "      <td>-0.042552</td>\n",
       "      <td>-0.147070</td>\n",
       "      <td>0.200573</td>\n",
       "      <td>0.060148</td>\n",
       "      <td>-0.022853</td>\n",
       "      <td>0.095694</td>\n",
       "      <td>0.162381</td>\n",
       "      <td>0.030324</td>\n",
       "      <td>0.004255</td>\n",
       "      <td>0.068805</td>\n",
       "      <td>0.030952</td>\n",
       "      <td>-0.146935</td>\n",
       "      <td>0.108283</td>\n",
       "      <td>0.048812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.081005</td>\n",
       "      <td>0.004228</td>\n",
       "      <td>-0.053044</td>\n",
       "      <td>0.040520</td>\n",
       "      <td>0.024845</td>\n",
       "      <td>-0.008541</td>\n",
       "      <td>0.179885</td>\n",
       "      <td>0.056030</td>\n",
       "      <td>-0.036178</td>\n",
       "      <td>0.012456</td>\n",
       "      <td>0.044646</td>\n",
       "      <td>-0.046362</td>\n",
       "      <td>-0.058848</td>\n",
       "      <td>-0.065474</td>\n",
       "      <td>-0.073071</td>\n",
       "      <td>0.147705</td>\n",
       "      <td>-0.005530</td>\n",
       "      <td>-0.028087</td>\n",
       "      <td>-0.045952</td>\n",
       "      <td>0.049049</td>\n",
       "      <td>-0.002582</td>\n",
       "      <td>-0.008430</td>\n",
       "      <td>-0.073209</td>\n",
       "      <td>0.073207</td>\n",
       "      <td>-0.107293</td>\n",
       "      <td>0.103643</td>\n",
       "      <td>0.006630</td>\n",
       "      <td>-0.052113</td>\n",
       "      <td>0.079209</td>\n",
       "      <td>0.060558</td>\n",
       "      <td>-0.042594</td>\n",
       "      <td>0.041347</td>\n",
       "      <td>-0.104618</td>\n",
       "      <td>-0.068099</td>\n",
       "      <td>0.040426</td>\n",
       "      <td>0.021027</td>\n",
       "      <td>0.181957</td>\n",
       "      <td>-0.093084</td>\n",
       "      <td>-0.110650</td>\n",
       "      <td>0.033879</td>\n",
       "      <td>-0.117493</td>\n",
       "      <td>-0.068428</td>\n",
       "      <td>-0.067708</td>\n",
       "      <td>0.023730</td>\n",
       "      <td>0.008597</td>\n",
       "      <td>-0.035992</td>\n",
       "      <td>0.016599</td>\n",
       "      <td>0.030511</td>\n",
       "      <td>0.195763</td>\n",
       "      <td>0.029416</td>\n",
       "      <td>-0.101782</td>\n",
       "      <td>-0.032113</td>\n",
       "      <td>-0.186283</td>\n",
       "      <td>0.007604</td>\n",
       "      <td>-0.039727</td>\n",
       "      <td>0.013488</td>\n",
       "      <td>-0.028124</td>\n",
       "      <td>0.073820</td>\n",
       "      <td>-0.198047</td>\n",
       "      <td>-0.012959</td>\n",
       "      <td>-0.099689</td>\n",
       "      <td>-0.034324</td>\n",
       "      <td>-0.015093</td>\n",
       "      <td>0.005853</td>\n",
       "      <td>-0.025376</td>\n",
       "      <td>0.181673</td>\n",
       "      <td>-0.087652</td>\n",
       "      <td>-0.018117</td>\n",
       "      <td>0.050967</td>\n",
       "      <td>0.077971</td>\n",
       "      <td>-0.226367</td>\n",
       "      <td>-0.038977</td>\n",
       "      <td>0.033673</td>\n",
       "      <td>-0.021974</td>\n",
       "      <td>-0.165693</td>\n",
       "      <td>0.055661</td>\n",
       "      <td>0.095474</td>\n",
       "      <td>0.021724</td>\n",
       "      <td>0.069713</td>\n",
       "      <td>0.036554</td>\n",
       "      <td>-0.033836</td>\n",
       "      <td>-0.084823</td>\n",
       "      <td>-0.006852</td>\n",
       "      <td>-0.018747</td>\n",
       "      <td>-0.050216</td>\n",
       "      <td>0.054739</td>\n",
       "      <td>0.045196</td>\n",
       "      <td>0.191172</td>\n",
       "      <td>-0.015382</td>\n",
       "      <td>0.236838</td>\n",
       "      <td>0.109173</td>\n",
       "      <td>0.056431</td>\n",
       "      <td>-0.026758</td>\n",
       "      <td>0.085752</td>\n",
       "      <td>0.025446</td>\n",
       "      <td>0.166266</td>\n",
       "      <td>0.029647</td>\n",
       "      <td>-0.124076</td>\n",
       "      <td>0.097932</td>\n",
       "      <td>0.151055</td>\n",
       "      <td>0.082890</td>\n",
       "      <td>0.046979</td>\n",
       "      <td>0.057959</td>\n",
       "      <td>-0.088994</td>\n",
       "      <td>-0.019969</td>\n",
       "      <td>-0.041011</td>\n",
       "      <td>-0.164735</td>\n",
       "      <td>0.071074</td>\n",
       "      <td>-0.000972</td>\n",
       "      <td>-0.045036</td>\n",
       "      <td>0.027100</td>\n",
       "      <td>0.078719</td>\n",
       "      <td>-0.078794</td>\n",
       "      <td>0.043396</td>\n",
       "      <td>0.043838</td>\n",
       "      <td>-0.155060</td>\n",
       "      <td>0.175683</td>\n",
       "      <td>0.152753</td>\n",
       "      <td>-0.023194</td>\n",
       "      <td>0.044113</td>\n",
       "      <td>0.162540</td>\n",
       "      <td>-0.036762</td>\n",
       "      <td>-0.014564</td>\n",
       "      <td>0.092364</td>\n",
       "      <td>-0.087684</td>\n",
       "      <td>-0.120937</td>\n",
       "      <td>0.202286</td>\n",
       "      <td>-0.008139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.123925</td>\n",
       "      <td>-0.052066</td>\n",
       "      <td>0.138516</td>\n",
       "      <td>0.071824</td>\n",
       "      <td>0.129243</td>\n",
       "      <td>-0.162270</td>\n",
       "      <td>0.139032</td>\n",
       "      <td>-0.001358</td>\n",
       "      <td>0.152213</td>\n",
       "      <td>0.068851</td>\n",
       "      <td>-0.019834</td>\n",
       "      <td>-0.091487</td>\n",
       "      <td>-0.024753</td>\n",
       "      <td>-0.131013</td>\n",
       "      <td>-0.113271</td>\n",
       "      <td>0.037514</td>\n",
       "      <td>0.060303</td>\n",
       "      <td>0.044764</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>0.083523</td>\n",
       "      <td>0.127654</td>\n",
       "      <td>-0.055403</td>\n",
       "      <td>-0.032948</td>\n",
       "      <td>0.062515</td>\n",
       "      <td>0.212051</td>\n",
       "      <td>-0.086504</td>\n",
       "      <td>0.082693</td>\n",
       "      <td>-0.041964</td>\n",
       "      <td>0.008191</td>\n",
       "      <td>0.021028</td>\n",
       "      <td>-0.021348</td>\n",
       "      <td>0.053067</td>\n",
       "      <td>0.023826</td>\n",
       "      <td>-0.053263</td>\n",
       "      <td>0.138226</td>\n",
       "      <td>0.034548</td>\n",
       "      <td>-0.003407</td>\n",
       "      <td>-0.066503</td>\n",
       "      <td>-0.066560</td>\n",
       "      <td>0.201065</td>\n",
       "      <td>-0.044213</td>\n",
       "      <td>-0.099786</td>\n",
       "      <td>-0.111227</td>\n",
       "      <td>0.112045</td>\n",
       "      <td>0.023470</td>\n",
       "      <td>0.029281</td>\n",
       "      <td>-0.095862</td>\n",
       "      <td>0.104739</td>\n",
       "      <td>0.138966</td>\n",
       "      <td>0.017368</td>\n",
       "      <td>-0.009061</td>\n",
       "      <td>-0.094560</td>\n",
       "      <td>-0.090627</td>\n",
       "      <td>0.074103</td>\n",
       "      <td>-0.086250</td>\n",
       "      <td>0.049916</td>\n",
       "      <td>0.004081</td>\n",
       "      <td>-0.098276</td>\n",
       "      <td>-0.092278</td>\n",
       "      <td>-0.026923</td>\n",
       "      <td>0.030280</td>\n",
       "      <td>0.035743</td>\n",
       "      <td>0.058266</td>\n",
       "      <td>-0.002155</td>\n",
       "      <td>-0.016706</td>\n",
       "      <td>0.129651</td>\n",
       "      <td>-0.090009</td>\n",
       "      <td>-0.065476</td>\n",
       "      <td>0.092167</td>\n",
       "      <td>0.080453</td>\n",
       "      <td>-0.020104</td>\n",
       "      <td>0.058897</td>\n",
       "      <td>-0.069826</td>\n",
       "      <td>-0.009155</td>\n",
       "      <td>0.079503</td>\n",
       "      <td>0.202416</td>\n",
       "      <td>0.019736</td>\n",
       "      <td>-0.102979</td>\n",
       "      <td>-0.016870</td>\n",
       "      <td>-0.048642</td>\n",
       "      <td>0.014475</td>\n",
       "      <td>0.066373</td>\n",
       "      <td>0.028614</td>\n",
       "      <td>0.023247</td>\n",
       "      <td>0.092261</td>\n",
       "      <td>0.141972</td>\n",
       "      <td>0.058337</td>\n",
       "      <td>0.093477</td>\n",
       "      <td>0.088508</td>\n",
       "      <td>0.173028</td>\n",
       "      <td>0.046415</td>\n",
       "      <td>0.061534</td>\n",
       "      <td>-0.096793</td>\n",
       "      <td>0.036895</td>\n",
       "      <td>0.053152</td>\n",
       "      <td>-0.011135</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.040696</td>\n",
       "      <td>0.022206</td>\n",
       "      <td>0.108058</td>\n",
       "      <td>0.180483</td>\n",
       "      <td>0.030415</td>\n",
       "      <td>-0.030597</td>\n",
       "      <td>-0.095530</td>\n",
       "      <td>-0.035163</td>\n",
       "      <td>-0.089695</td>\n",
       "      <td>-0.102284</td>\n",
       "      <td>0.092725</td>\n",
       "      <td>-0.109420</td>\n",
       "      <td>-0.034426</td>\n",
       "      <td>0.168165</td>\n",
       "      <td>0.022106</td>\n",
       "      <td>-0.139659</td>\n",
       "      <td>0.165264</td>\n",
       "      <td>-0.136218</td>\n",
       "      <td>0.012064</td>\n",
       "      <td>0.084255</td>\n",
       "      <td>0.117795</td>\n",
       "      <td>-0.073345</td>\n",
       "      <td>0.118273</td>\n",
       "      <td>0.101300</td>\n",
       "      <td>-0.055486</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>0.068995</td>\n",
       "      <td>-0.025478</td>\n",
       "      <td>-0.056114</td>\n",
       "      <td>0.200029</td>\n",
       "      <td>0.029064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.293592</td>\n",
       "      <td>-0.056428</td>\n",
       "      <td>-0.024551</td>\n",
       "      <td>0.071801</td>\n",
       "      <td>-0.071654</td>\n",
       "      <td>-0.116774</td>\n",
       "      <td>-0.023651</td>\n",
       "      <td>0.003016</td>\n",
       "      <td>0.156659</td>\n",
       "      <td>-0.056435</td>\n",
       "      <td>-0.046234</td>\n",
       "      <td>-0.209807</td>\n",
       "      <td>-0.100383</td>\n",
       "      <td>-0.155047</td>\n",
       "      <td>-0.087509</td>\n",
       "      <td>-0.012929</td>\n",
       "      <td>-0.012676</td>\n",
       "      <td>0.106753</td>\n",
       "      <td>-0.178907</td>\n",
       "      <td>0.164996</td>\n",
       "      <td>0.066246</td>\n",
       "      <td>0.044185</td>\n",
       "      <td>0.120277</td>\n",
       "      <td>-0.039545</td>\n",
       "      <td>0.127516</td>\n",
       "      <td>-0.017064</td>\n",
       "      <td>0.043701</td>\n",
       "      <td>-0.050214</td>\n",
       "      <td>0.017625</td>\n",
       "      <td>0.114384</td>\n",
       "      <td>-0.068798</td>\n",
       "      <td>-0.036497</td>\n",
       "      <td>0.031297</td>\n",
       "      <td>-0.037767</td>\n",
       "      <td>0.072512</td>\n",
       "      <td>0.070644</td>\n",
       "      <td>-0.093135</td>\n",
       "      <td>-0.033789</td>\n",
       "      <td>-0.105243</td>\n",
       "      <td>0.036780</td>\n",
       "      <td>-0.111621</td>\n",
       "      <td>-0.076665</td>\n",
       "      <td>0.017004</td>\n",
       "      <td>0.094211</td>\n",
       "      <td>-0.045597</td>\n",
       "      <td>-0.091296</td>\n",
       "      <td>-0.104439</td>\n",
       "      <td>-0.285935</td>\n",
       "      <td>-0.005899</td>\n",
       "      <td>-0.011326</td>\n",
       "      <td>-0.003503</td>\n",
       "      <td>-0.051269</td>\n",
       "      <td>-0.130770</td>\n",
       "      <td>0.097393</td>\n",
       "      <td>-0.057601</td>\n",
       "      <td>0.099895</td>\n",
       "      <td>0.052592</td>\n",
       "      <td>-0.061070</td>\n",
       "      <td>0.059657</td>\n",
       "      <td>-0.019298</td>\n",
       "      <td>-0.003554</td>\n",
       "      <td>-0.139596</td>\n",
       "      <td>0.010336</td>\n",
       "      <td>-0.032279</td>\n",
       "      <td>0.044470</td>\n",
       "      <td>-0.003107</td>\n",
       "      <td>-0.038024</td>\n",
       "      <td>-0.232843</td>\n",
       "      <td>-0.006948</td>\n",
       "      <td>0.106289</td>\n",
       "      <td>-0.041557</td>\n",
       "      <td>0.080512</td>\n",
       "      <td>-0.106400</td>\n",
       "      <td>-0.135224</td>\n",
       "      <td>0.083332</td>\n",
       "      <td>0.005111</td>\n",
       "      <td>-0.136972</td>\n",
       "      <td>-0.091987</td>\n",
       "      <td>-0.077191</td>\n",
       "      <td>-0.027330</td>\n",
       "      <td>-0.085932</td>\n",
       "      <td>0.092863</td>\n",
       "      <td>-0.088209</td>\n",
       "      <td>0.003616</td>\n",
       "      <td>0.029883</td>\n",
       "      <td>0.068965</td>\n",
       "      <td>-0.018657</td>\n",
       "      <td>0.085188</td>\n",
       "      <td>0.327313</td>\n",
       "      <td>0.347952</td>\n",
       "      <td>0.005294</td>\n",
       "      <td>0.013236</td>\n",
       "      <td>-0.009080</td>\n",
       "      <td>-0.043094</td>\n",
       "      <td>-0.098621</td>\n",
       "      <td>0.109728</td>\n",
       "      <td>-0.023589</td>\n",
       "      <td>-0.007549</td>\n",
       "      <td>0.003626</td>\n",
       "      <td>0.055322</td>\n",
       "      <td>0.062790</td>\n",
       "      <td>0.102514</td>\n",
       "      <td>0.068060</td>\n",
       "      <td>0.049274</td>\n",
       "      <td>0.124404</td>\n",
       "      <td>-0.135533</td>\n",
       "      <td>-0.131421</td>\n",
       "      <td>-0.058870</td>\n",
       "      <td>-0.062151</td>\n",
       "      <td>0.023403</td>\n",
       "      <td>0.014734</td>\n",
       "      <td>-0.048934</td>\n",
       "      <td>0.071324</td>\n",
       "      <td>-0.014838</td>\n",
       "      <td>-0.091412</td>\n",
       "      <td>-0.087450</td>\n",
       "      <td>-0.014189</td>\n",
       "      <td>-0.003481</td>\n",
       "      <td>-0.071550</td>\n",
       "      <td>-0.060644</td>\n",
       "      <td>0.026855</td>\n",
       "      <td>-0.067761</td>\n",
       "      <td>-0.059928</td>\n",
       "      <td>0.128818</td>\n",
       "      <td>0.036933</td>\n",
       "      <td>0.031695</td>\n",
       "      <td>0.049605</td>\n",
       "      <td>-0.023125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.225497 -0.097742  0.230630  0.139735  0.086153 -0.045874  0.071118   \n",
       "1  0.081005  0.004228 -0.053044  0.040520  0.024845 -0.008541  0.179885   \n",
       "2  0.123925 -0.052066  0.138516  0.071824  0.129243 -0.162270  0.139032   \n",
       "3  0.293592 -0.056428 -0.024551  0.071801 -0.071654 -0.116774 -0.023651   \n",
       "\n",
       "        7         8         9         10        11        12        13   \\\n",
       "0 -0.160892  0.112284  0.011471 -0.007685 -0.075731 -0.092246 -0.120212   \n",
       "1  0.056030 -0.036178  0.012456  0.044646 -0.046362 -0.058848 -0.065474   \n",
       "2 -0.001358  0.152213  0.068851 -0.019834 -0.091487 -0.024753 -0.131013   \n",
       "3  0.003016  0.156659 -0.056435 -0.046234 -0.209807 -0.100383 -0.155047   \n",
       "\n",
       "        14        15        16        17        18        19        20   \\\n",
       "0 -0.040878  0.188590  0.045904  0.006215 -0.124716  0.082918  0.118537   \n",
       "1 -0.073071  0.147705 -0.005530 -0.028087 -0.045952  0.049049 -0.002582   \n",
       "2 -0.113271  0.037514  0.060303  0.044764  0.000226  0.083523  0.127654   \n",
       "3 -0.087509 -0.012929 -0.012676  0.106753 -0.178907  0.164996  0.066246   \n",
       "\n",
       "        21        22        23        24        25        26        27   \\\n",
       "0  0.050865 -0.061020 -0.034868  0.024469  0.085191  0.109251 -0.024774   \n",
       "1 -0.008430 -0.073209  0.073207 -0.107293  0.103643  0.006630 -0.052113   \n",
       "2 -0.055403 -0.032948  0.062515  0.212051 -0.086504  0.082693 -0.041964   \n",
       "3  0.044185  0.120277 -0.039545  0.127516 -0.017064  0.043701 -0.050214   \n",
       "\n",
       "        28        29        30        31        32        33        34   \\\n",
       "0  0.011921  0.146256 -0.037281  0.080321 -0.095374 -0.043692  0.136369   \n",
       "1  0.079209  0.060558 -0.042594  0.041347 -0.104618 -0.068099  0.040426   \n",
       "2  0.008191  0.021028 -0.021348  0.053067  0.023826 -0.053263  0.138226   \n",
       "3  0.017625  0.114384 -0.068798 -0.036497  0.031297 -0.037767  0.072512   \n",
       "\n",
       "        35        36        37        38        39        40        41   \\\n",
       "0  0.042527  0.053423 -0.100668 -0.034157  0.115415 -0.135856 -0.059653   \n",
       "1  0.021027  0.181957 -0.093084 -0.110650  0.033879 -0.117493 -0.068428   \n",
       "2  0.034548 -0.003407 -0.066503 -0.066560  0.201065 -0.044213 -0.099786   \n",
       "3  0.070644 -0.093135 -0.033789 -0.105243  0.036780 -0.111621 -0.076665   \n",
       "\n",
       "        42        43        44        45        46        47        48   \\\n",
       "0 -0.008103  0.105087  0.045498 -0.033328 -0.006868  0.004209  0.091305   \n",
       "1 -0.067708  0.023730  0.008597 -0.035992  0.016599  0.030511  0.195763   \n",
       "2 -0.111227  0.112045  0.023470  0.029281 -0.095862  0.104739  0.138966   \n",
       "3  0.017004  0.094211 -0.045597 -0.091296 -0.104439 -0.285935 -0.005899   \n",
       "\n",
       "        49        50        51        52        53        54        55   \\\n",
       "0 -0.003479 -0.033370  0.105743 -0.112079  0.068110 -0.041359 -0.018282   \n",
       "1  0.029416 -0.101782 -0.032113 -0.186283  0.007604 -0.039727  0.013488   \n",
       "2  0.017368 -0.009061 -0.094560 -0.090627  0.074103 -0.086250  0.049916   \n",
       "3 -0.011326 -0.003503 -0.051269 -0.130770  0.097393 -0.057601  0.099895   \n",
       "\n",
       "        56        57        58        59        60        61        62   \\\n",
       "0 -0.132670  0.016037 -0.096326 -0.003783 -0.059288 -0.059012  0.078189   \n",
       "1 -0.028124  0.073820 -0.198047 -0.012959 -0.099689 -0.034324 -0.015093   \n",
       "2  0.004081 -0.098276 -0.092278 -0.026923  0.030280  0.035743  0.058266   \n",
       "3  0.052592 -0.061070  0.059657 -0.019298 -0.003554 -0.139596  0.010336   \n",
       "\n",
       "        63        64        65        66        67        68        69   \\\n",
       "0 -0.089968  0.046925  0.117595 -0.164911 -0.042599  0.097080  0.028123   \n",
       "1  0.005853 -0.025376  0.181673 -0.087652 -0.018117  0.050967  0.077971   \n",
       "2 -0.002155 -0.016706  0.129651 -0.090009 -0.065476  0.092167  0.080453   \n",
       "3 -0.032279  0.044470 -0.003107 -0.038024 -0.232843 -0.006948  0.106289   \n",
       "\n",
       "        70        71        72        73        74        75        76   \\\n",
       "0 -0.200099  0.088148  0.027126 -0.029836  0.045751  0.100097  0.051409   \n",
       "1 -0.226367 -0.038977  0.033673 -0.021974 -0.165693  0.055661  0.095474   \n",
       "2 -0.020104  0.058897 -0.069826 -0.009155  0.079503  0.202416  0.019736   \n",
       "3 -0.041557  0.080512 -0.106400 -0.135224  0.083332  0.005111 -0.136972   \n",
       "\n",
       "        77        78        79        80        81        82        83   \\\n",
       "0 -0.131577  0.039602 -0.101337  0.018500 -0.029483  0.025566 -0.152488   \n",
       "1  0.021724  0.069713  0.036554 -0.033836 -0.084823 -0.006852 -0.018747   \n",
       "2 -0.102979 -0.016870 -0.048642  0.014475  0.066373  0.028614  0.023247   \n",
       "3 -0.091987 -0.077191 -0.027330 -0.085932  0.092863 -0.088209  0.003616   \n",
       "\n",
       "        84        85        86        87        88        89        90   \\\n",
       "0  0.136041 -0.041233 -0.017325  0.068525  0.108027  0.191753  0.034966   \n",
       "1 -0.050216  0.054739  0.045196  0.191172 -0.015382  0.236838  0.109173   \n",
       "2  0.092261  0.141972  0.058337  0.093477  0.088508  0.173028  0.046415   \n",
       "3  0.029883  0.068965 -0.018657  0.085188  0.327313  0.347952  0.005294   \n",
       "\n",
       "        91        92        93        94        95        96        97   \\\n",
       "0  0.055814 -0.058237 -0.012290  0.030932  0.137788  0.047096 -0.037614   \n",
       "1  0.056431 -0.026758  0.085752  0.025446  0.166266  0.029647 -0.124076   \n",
       "2  0.061534 -0.096793  0.036895  0.053152 -0.011135  0.000003 -0.040696   \n",
       "3  0.013236 -0.009080 -0.043094 -0.098621  0.109728 -0.023589 -0.007549   \n",
       "\n",
       "        98        99        100       101       102       103       104  \\\n",
       "0 -0.042310  0.137921  0.164177  0.104449  0.124679 -0.109762 -0.015248   \n",
       "1  0.097932  0.151055  0.082890  0.046979  0.057959 -0.088994 -0.019969   \n",
       "2  0.022206  0.108058  0.180483  0.030415 -0.030597 -0.095530 -0.035163   \n",
       "3  0.003626  0.055322  0.062790  0.102514  0.068060  0.049274  0.124404   \n",
       "\n",
       "        105       106       107       108       109       110       111  \\\n",
       "0 -0.188383 -0.210898  0.028174 -0.034462 -0.093645 -0.068922 -0.084514   \n",
       "1 -0.041011 -0.164735  0.071074 -0.000972 -0.045036  0.027100  0.078719   \n",
       "2 -0.089695 -0.102284  0.092725 -0.109420 -0.034426  0.168165  0.022106   \n",
       "3 -0.135533 -0.131421 -0.058870 -0.062151  0.023403  0.014734 -0.048934   \n",
       "\n",
       "        112       113       114       115       116       117       118  \\\n",
       "0 -0.024698  0.107229 -0.042552 -0.147070  0.200573  0.060148 -0.022853   \n",
       "1 -0.078794  0.043396  0.043838 -0.155060  0.175683  0.152753 -0.023194   \n",
       "2 -0.139659  0.165264 -0.136218  0.012064  0.084255  0.117795 -0.073345   \n",
       "3  0.071324 -0.014838 -0.091412 -0.087450 -0.014189 -0.003481 -0.071550   \n",
       "\n",
       "        119       120       121       122       123       124       125  \\\n",
       "0  0.095694  0.162381  0.030324  0.004255  0.068805  0.030952 -0.146935   \n",
       "1  0.044113  0.162540 -0.036762 -0.014564  0.092364 -0.087684 -0.120937   \n",
       "2  0.118273  0.101300 -0.055486 -0.043401  0.068995 -0.025478 -0.056114   \n",
       "3 -0.060644  0.026855 -0.067761 -0.059928  0.128818  0.036933  0.031695   \n",
       "\n",
       "        126       127  \n",
       "0  0.108283  0.048812  \n",
       "1  0.202286 -0.008139  \n",
       "2  0.200029  0.029064  \n",
       "3  0.049605 -0.023125  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statements = [\n",
    "    'very bad',\n",
    "    'atrocious',\n",
    "    'good',\n",
    "    'this is interesting',\n",
    "]\n",
    "\n",
    "embed = hub.load('https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1')\n",
    "embeddings = embed(statements)\n",
    "embeddings = pd.DataFrame(embeddings)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to train.py\n"
     ]
    }
   ],
   "source": [
    "# %%writefile -a train.py \n",
    "\n",
    "def train(EPOCHS=8, BATCH_SIZE=8, TRAIN_FILE='train.csv', VAL_FILE='test.csv'):\n",
    "    WORKDIR = os.getcwd()\n",
    "    print(\"Loading train and val data...\")\n",
    "    y_train, x_train = load_dataset(TRAIN_FILE, num_samples=100000)\n",
    "    y_val, x_val     = load_dataset(VAL_FILE,   num_samples=10000)\n",
    "\n",
    "    print(\"Training the model...\")\n",
    "    model = get_model()\n",
    "    model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "              verbose=1, validation_data=(x_val, y_val),\n",
    "              callbacks=[tf.keras.callbacks.ModelCheckpoint(os.path.join(WORKDIR, 'model_checkpoint'), monitor='val_loss', verbose=1, save_best_model=True, save_weights_only=False, mode='auto')])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def export_model(model, base_path='amazon_review/'):\n",
    "    path = os.path.join(base_path, str(int(time.time())))\n",
    "    tf.saved_model.save(model, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to train.py\n"
     ]
    }
   ],
   "source": [
    "# %%writefile -a train.py \n",
    "\n",
    "###? TRAIN AND EXPORT MODEL AS PROTOBUF\n",
    "if __name__=='__main__':\n",
    "    model = train(TRAIN_FILE='./data/amazonfinefoodreviews/train.csv', VAL_FILE='./data/amazonfinefoodreviews/test.csv')\n",
    "    export_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Inference Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[0.64, 0.09, 0.27]], dtype=float32),\n",
       " array([[0.91, 0.00, 0.09]], dtype=float32),\n",
       " array([[0.01, 0.00, 0.98]], dtype=float32)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    'waste of time.',\n",
    "    'this sucks',\n",
    "    'brilliant, i love it',\n",
    "]\n",
    "\n",
    "[ model.predict([t]) for t in test_sentences ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying the model\n",
    "- Run the following in CLI.\n",
    "  \n",
    "        docker pull tensorflow/serving:2.8.0\n",
    "\n",
    "        docker run -p 8500:8500 -p 8501:8501 \\\n",
    "            --mount type=bind,source=d:/sentimentanalysis/amazon_review/,target=/models/am_rvw \\\n",
    "            -e MODEL_NAME=am_rvw \\\n",
    "            -t tensorflow/serving:2.8.0\n",
    "\n",
    "        (sidenote, this command also accomplishes the same:)\n",
    "        docker run -p 8500:8500 -p 8501:8501 -v d:/sentimentanalysis/amazon_review:/models/am_rvw -e MODEL_NAME=am_rvw -t tensorflow/serving:2.8.0\n",
    "\n",
    "- port `8500` is for the **gRPC API**, and `8501` is for **REST API**. The short of it is, REST API is easier/convenient, but gRPC is more performant, efficient.\n",
    "            \n",
    "- in my case, training and building on tensorflow 2.9.0, I had to deploy on tensorflow/serving:2.8.0 otherwise I kept getting the following crash during deployment/inference:\n",
    "  - tensorflow-serving      | terminate called after throwing an instance of 'std::bad_alloc'\n",
    "  - tensorflow-serving      |   what():  std::bad_alloc\n",
    "  - which seemed to be an out-of-memory thing according to most threads, however even after increasing my WSL2 backend memory=20GB and swap=4GB, it would still crash, and Docker stats showed the container only around 600mb. A day of trial and erroring around with tf versions I finally found the above combination to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference using REST and curl from command line:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in windows, you have to escape the quotes\n",
    "  - `curl -d \"{\\\"instances\\\": [\\\"this sucks\\\"]}\" -X POST http://127.0.0.1:8501/v1/models/am_rvw:predict`\n",
    "- linux/mac\n",
    "  - `curl -d '{\"instances\": [\"this sucks\"]}' -X POST http://127.0.0.1:8501/v1/models/am_rvw:predict`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "endpoints here:\n",
    "\n",
    "        # LATEST MODEL\n",
    "        http://localhost:8501/v1/models/amrvw\n",
    "        # SPECIFIC MODEL\n",
    "        http://localhost:8501/v1/models/amrvw/versions/1:predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference using a python REST client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tf_serving_rest_client.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tf_serving_rest_client.py\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import sys\n",
    "\n",
    "def get_rest_url(model_name, host='127.0.0.1', port='8501', verb='predict', version=None):\n",
    "    \"\"\" generate the URL path\"\"\"\n",
    "    url = \"http://{host}:{port}/v1/models/{model_name}\".format(host=host, port=port, model_name=model_name)\n",
    "    if version:\n",
    "        url += 'versions/{version}'.format(version=version)\n",
    "    url += ':{verb}'.format(verb=verb)\n",
    "    return url\n",
    "\n",
    "def get_model_prediction(model_input, model_name='amrvw', signature_name='serving_default'):\n",
    "    \"\"\" no error handling at all, just poc\"\"\"\n",
    "\n",
    "    url = get_rest_url(model_name)\n",
    "    #In the row format, inputs are keyed to instances key in the JSON request.\n",
    "    #When there is only one named input, specify the value of instances key to be the value of the input:\n",
    "    \n",
    "    # in our case, no difference between using \"instances\" or \"inputs\".\n",
    "    # data = {\"instances\": [model_input]}\n",
    "    data = {\"inputs\": [model_input]}\n",
    "    \n",
    "    rv = requests.post(url, data=json.dumps(data))\n",
    "    if rv.status_code != requests.codes.ok:\n",
    "        rv.raise_for_status()\n",
    "    \n",
    "    return rv.json()['predictions']\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    print(\"\\nGenerate REST url ...\")\n",
    "    url = get_rest_url(model_name='amrvw')\n",
    "    print(url)\n",
    "    \n",
    "    while True:\n",
    "        print(\"\\nEnter an Amazon review [:q for Quit]\")\n",
    "        if sys.version_info[0] <= 3:\n",
    "            sentence = input()\n",
    "        if sentence == ':q':\n",
    "            break\n",
    "        model_input = sentence\n",
    "        model_prediction = get_model_prediction(model_input)\n",
    "        print(\"The model predicted ...\")\n",
    "        print(model_prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference using a gRPC client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tf_serving_grpc_client.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tf_serving_grpc_client.py\n",
    "import sys\n",
    "import grpc\n",
    "from grpc.beta import implementations\n",
    "import tensorflow as tf\n",
    "from tensorflow_serving.apis import predict_pb2\n",
    "from tensorflow_serving.apis import prediction_service_pb2, get_model_metadata_pb2\n",
    "from tensorflow_serving.apis import prediction_service_pb2_grpc\n",
    "\n",
    "\n",
    "def get_stub(host='127.0.0.1', port='8500'):\n",
    "    channel = grpc.insecure_channel('127.0.0.1:8500') \n",
    "    stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n",
    "    return stub\n",
    "\n",
    "\n",
    "def get_model_prediction(model_input, stub, model_name='amrvw', signature_name='serving_default'):\n",
    "    \"\"\" no error handling at all, just poc\"\"\"\n",
    "    request = predict_pb2.PredictRequest()\n",
    "    request.model_spec.name = model_name\n",
    "    request.model_spec.signature_name = signature_name\n",
    "    request.inputs['input_input'].CopyFrom(tf.make_tensor_proto(model_input))\n",
    "    response = stub.Predict.future(request, 5.0)  # 5 seconds\n",
    "    return response.result().outputs[\"output\"].float_val\n",
    "\n",
    "\n",
    "def get_model_version(model_name, stub):\n",
    "    request = get_model_metadata_pb2.GetModelMetadataRequest()\n",
    "    request.model_spec.name = 'amrvw'\n",
    "    request.metadata_field.append(\"signature_def\")\n",
    "    response = stub.GetModelMetadata(request, 10)\n",
    "    # signature of loaded model is available here: response.metadata['signature_def']\n",
    "    return response.model_spec.version.value\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"\\nCreate RPC connection ...\")\n",
    "    stub = get_stub()\n",
    "    while True:\n",
    "        print(\"\\nEnter an Amazon review [:q for Quit]\")\n",
    "        if sys.version_info[0] <= 3:\n",
    "            sentence = raw_input() if sys.version_info[0] < 3 else input()\n",
    "        if sentence == ':q':\n",
    "            break\n",
    "        model_input = [sentence]\n",
    "        model_prediction = get_model_prediction(model_input, stub)\n",
    "        print(\"The model predicted ...\")\n",
    "        print(model_prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tf260')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b88a46baaec1e77d858bff68f0da0e931da80f0ae90bb1780274e5ea04ccf25f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
